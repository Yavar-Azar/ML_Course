{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca606dc",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a statistical method used to model the relationship between a **dependent variable** (the outcome you're trying to predict) and one or more **independent variables** (the predictors or factors that influence the outcome). The goal of linear regression is to find a mathematical function that maps the independent variables to the dependent variable as accurately as possible.\n",
    "\n",
    "### Variables in Linear Regression\n",
    "\n",
    "- The dependent variable is often denoted as $y$, which represents the output or response.\n",
    "- The independent variables are represented as $x_1, x_2, \\dots, x_r$, where $r$ is the number of independent variables or predictors.\n",
    "\n",
    "If we have multiple independent variables, we can group them into a vector $\\mathbf{x} = (x_1, x_2, \\dots, x_r)$. \n",
    "\n",
    "### The Linear Regression Model\n",
    "\n",
    "Linear regression assumes a **linear relationship** between the dependent variable $y$ and the independent variables $\\mathbf{x}$. This relationship can be expressed by the following equation, known as the **regression equation**:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_r x_r + \\varepsilon\n",
    "$$\n",
    "\n",
    "- $\\beta_0$ is the **intercept**, representing the value of $y$ when all $x$ values are zero.\n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_r$ are the **regression coefficients**, which indicate how much $y$ changes with a one-unit change in each corresponding $x_i$.\n",
    "- $\\varepsilon$ is the **error term** (or residual), accounting for any variability in $y$ that cannot be explained by the linear relationship with $x_1, \\dots, x_r$.\n",
    "\n",
    "### Estimating the Regression Function\n",
    "\n",
    "To estimate the coefficients $\\beta_0, \\beta_1, \\dots, \\beta_r$ from the data, we calculate their **estimators**, which are typically denoted as $b_0, b_1, \\dots, b_r$. Using these estimators, we can write the **estimated regression function** as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = b_0 + b_1 x_1 + \\cdots + b_r x_r\n",
    "$$\n",
    "\n",
    "This function predicts the value of $y$ based on the input values of $x_1, \\dots, x_r$. The goal of linear regression is to find the values of $b_0, b_1, \\dots, b_r$ that result in the best possible predictions.\n",
    "\n",
    "### Residuals and Error Minimization\n",
    "\n",
    "For each observation $i$, we can compute the **predicted response** $f(\\mathbf{x}_i)$, and compare it to the actual response $y_i$. The difference between them is called the **residual**:\n",
    "\n",
    "$$\n",
    "\\text{Residual} = y_i - f(\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "The smaller the residuals, the better the model fits the data. Therefore, linear regression aims to find the coefficients $b_0, b_1, \\dots, b_r$ that minimize the overall error, often measured as the **sum of squared residuals (SSR)**:\n",
    "\n",
    "$$\n",
    "SSR = \\sum_{i=1}^{n} \\left( y_i - f(\\mathbf{x}_i) \\right)^2\n",
    "$$\n",
    "\n",
    "This method of minimizing the SSR to determine the best-fitting line is known as the **ordinary least squares (OLS)** method.\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "**Multiple linear regression** is an extension of simple linear regression when you have more than one independent variable. The relationship is still linear, but now with multiple predictors influencing the dependent variable.\n",
    "\n",
    "For example, if there are two independent variables $x_1$ and $x_2$, the estimated regression function is:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = b_0 + b_1 x_1 + b_2 x_2\n",
    "$$\n",
    "\n",
    "This equation represents a **regression plane** in three-dimensional space. The goal is to find the values of $b_0, b_1$, and $b_2$ that make the plane fit the data points as closely as possible.\n",
    "\n",
    "In general, for $r$ independent variables, the regression equation becomes:\n",
    "\n",
    "$$\n",
    "f(x_1, \\dots, x_r) = b_0 + b_1 x_1 + \\cdots + b_r x_r\n",
    "$$\n",
    "\n",
    "There are $r + 1$ unknown coefficients (including the intercept $b_0$), which need to be determined by minimizing the SSR.\n",
    "\n",
    "## Polynomial Regression\n",
    "\n",
    "**Polynomial regression** is a form of linear regression where the relationship between the dependent and independent variables is modeled as a polynomial. It extends linear regression by allowing for non-linear relationships between the variables, while still keeping the model linear in the coefficients.\n",
    "\n",
    "For example, in polynomial regression, the regression function may include terms like:\n",
    "\n",
    "$$\n",
    "f(x_1) = b_0 + b_1 x_1 + b_2 x_1^2 + b_3 x_1^3\n",
    "$$\n",
    "\n",
    "Or even interaction terms such as:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = b_0 + b_1 x_1 + b_2 x_1^2 + b_3 x_1 x_2\n",
    "$$\n",
    "\n",
    "In this case, the model can capture more complex relationships by incorporating higher-degree terms or interactions between variables. While the model includes non-linear terms (such as $x_1^2$ or $x_1 x_2$), it is still considered a form of **linear regression** because the coefficients ($b_0, b_1, b_2, \\dots$) appear linearly.\n",
    "\n",
    "\n",
    "### Underfitting and Overfitting\n",
    "\n",
    "When implementing polynomial regression, a critical question arises: how do you choose the optimal degree for the polynomial regression function?\n",
    "\n",
    "Unfortunately, there is no universal rule for selecting the best degree—it largely depends on the specific problem and dataset. However, it's important to understand two common issues that can arise from selecting the wrong degree: **underfitting** and **overfitting**.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "**Underfitting** occurs when the model is too simple to capture the underlying patterns in the data. As a result, it fails to accurately represent the relationships between the variables. This often leads to a low $R^2$ score, indicating that the model explains very little of the variance in the data. Additionally, an underfit model will perform poorly when applied to both the training data and new, unseen data.\n",
    "\n",
    "Underfitting typically happens when the polynomial degree is too low, and the model lacks the complexity needed to capture the true behavior of the data.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "**Overfitting**, on the other hand, happens when the model becomes too complex and starts to learn not only the underlying data relationships but also random noise and fluctuations in the training data. This can result in a model that fits the training data almost perfectly, often yielding a very high $R^2$ score on the known data.\n",
    "\n",
    "However, the problem with overfitting is that the model doesn't generalize well to new, unseen data. While it performs excellently on the training data, its performance drops significantly when applied to new datasets, often resulting in a much lower $R^2$ score. Overfitting is common when the polynomial degree is too high, or when the model includes too many features or terms.\n",
    "\n",
    "### Finding the Right Balance\n",
    "\n",
    "To avoid both underfitting and overfitting, it's essential to find the right balance between model simplicity and complexity. Techniques like **cross-validation** and **regularization** can help in determining the optimal degree of the polynomial, ensuring that the model generalizes well to new data while accurately capturing the underlying relationships in the training data.\n",
    "\n",
    "\n",
    "\n",
    "## Regression with Scikit-Learn\n",
    "\n",
    "Let’s begin with the simplest scenario: **simple linear regression**. There are five key steps involved in implementing a linear regression model:\n",
    "\n",
    "1. **Import necessary packages and libraries**: Start by loading the tools and libraries you’ll need for the task.\n",
    "2. **Prepare and process your data**: Input the dataset you’ll be working with and apply any necessary transformations or preprocessing steps.\n",
    "3. **Build and train the regression model**: Create your linear regression model and train it using the available data.\n",
    "4. **Evaluate the model’s performance**: Check the model’s results to assess if it fits the data well and meets your expectations.\n",
    "5. **Make predictions with the model**: Once satisfied with the model, use it to make predictions on new data.\n",
    "\n",
    "These steps generally apply to most regression models, regardless of the specific approach or technique. As you proceed through this guide, you’ll see how to follow these steps for various types of regression scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a6ef3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T09:49:20.067596Z",
     "iopub.status.busy": "2024-10-08T09:49:20.066796Z",
     "iopub.status.idle": "2024-10-08T09:49:20.759549Z",
     "shell.execute_reply": "2024-10-08T09:49:20.758895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.7158756137479542\n",
      "intercept: 5.633333333333329\n",
      "slope: [0.54]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))\n",
    "y = np.array([5, 20, 14, 32, 22, 38])\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(x, y)\n",
    "\n",
    "r_sq = model.score(x, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "print(f\"intercept: {model.intercept_}\")\n",
    "\n",
    "\n",
    "print(f\"slope: {model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1efd1",
   "metadata": {},
   "source": [
    "### STEP 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa86f2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T09:49:20.762114Z",
     "iopub.status.busy": "2024-10-08T09:49:20.761803Z",
     "iopub.status.idle": "2024-10-08T09:49:20.768727Z",
     "shell.execute_reply": "2024-10-08T09:49:20.768106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.33333333, 13.73333333, 19.13333333, 24.53333333, 29.93333333,\n",
       "       35.33333333])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1571c0",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression With scikit-learn\n",
    "The main difference is that your x array will now have two or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e155f1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T09:49:20.771141Z",
     "iopub.status.busy": "2024-10-08T09:49:20.770906Z",
     "iopub.status.idle": "2024-10-08T09:49:20.778888Z",
     "shell.execute_reply": "2024-10-08T09:49:20.778034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.8615939258756776\n",
      "intercept: 5.52257927519819\n",
      "coefficients: [0.44706965 0.25502548]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x = [\n",
    "  [0, 1], [5, 1], [15, 2], [25, 5], [35, 11], [45, 15], [55, 34], [60, 35]\n",
    "]\n",
    "y = [4, 5, 20, 14, 32, 22, 38, 43]\n",
    "x, y = np.array(x), np.array(y)\n",
    "\n",
    "model = LinearRegression().fit(x, y)\n",
    "\n",
    "r_sq = model.score(x, y)\n",
    "print(f\"coefficient of determination: {r_sq}\")\n",
    "\n",
    "\n",
    "print(f\"intercept: {model.intercept_}\")\n",
    "\n",
    "\n",
    "print(f\"coefficients: {model.coef_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ca472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
