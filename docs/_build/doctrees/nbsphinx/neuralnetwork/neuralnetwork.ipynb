{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c82bce",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "A **neural network** is a type of machine learning model that learns to make predictions through a series of steps:\n",
    "\n",
    "1. **Receive input data**: The model takes in the raw data.\n",
    "2. **Make a prediction**: It uses the data to make an initial prediction.\n",
    "3. **Compare the prediction to the actual output**: The model checks how close its prediction was to the correct result.\n",
    "4. **Adjust its internal parameters**: Based on the comparison, the model updates its internal settings to improve future predictions.\n",
    "\n",
    "### Building Blocks of Neural Networks\n",
    "\n",
    "Neural networks are made up of several key components, including **vectors**, **layers**, and concepts from **linear regression**. Here’s how these work:\n",
    "\n",
    "- **Vectors**: Data is represented as vectors, which are simply lists of numbers. In Python, these vectors are usually stored in arrays.\n",
    "- **Layers**: A neural network consists of layers, where each layer processes the data it receives from the previous layer. You can think of each layer as a step that extracts more meaningful features from the data.\n",
    "- **Transformation of Data**: Each layer transforms the input in some way, gradually refining it to make better predictions. This process is similar to feature engineering, where each layer extracts a better representation of the data than the previous one.\n",
    "\n",
    "### General Applicability of Neural Networks\n",
    "\n",
    "One powerful feature of neural networks is that the same basic computations can be used on different types of data, whether it’s images, text, or something else. The process of extracting useful information and training the model is fundamentally the same across different data types.\n",
    "\n",
    "In the diagram below, you'll see a simple example of a neural network with two layers, illustrating how data moves through the layers to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eef4c5",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "Deep learning allows neural networks to automatically learn which features are important from raw data, eliminating the need for manual feature engineering. In traditional machine learning, feature engineering involves selecting or crafting specific attributes to improve predictions, but deep learning automates this process by discovering patterns through multiple layers of the network.\n",
    "\n",
    "This is particularly useful with complex datasets. For example, predicting a person’s mood from a photo would require defining facial features manually, but a deep learning model can learn these features on its own. Each layer of the network processes the image, extracting meaningful information to make accurate predictions without explicit guidance.\n",
    "\n",
    "In the next sections, we’ll explore how neural networks achieve this automatic feature learning.\n",
    "Explanation:\n",
    "\n",
    "    Deep learning automates feature extraction, which is a significant advantage over traditional machine learning, where feature engineering can be difficult.\n",
    "    Neural networks learn patterns through multiple layers, making them highly effective for complex tasks like image or speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa945285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T15:11:53.916104Z",
     "iopub.status.busy": "2024-10-16T15:11:53.915619Z",
     "iopub.status.idle": "2024-10-16T15:11:54.043930Z",
     "shell.execute_reply": "2024-10-16T15:11:54.043148Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with improved weight initialization.\n",
    "        \n",
    "        Parameters:\n",
    "        - layers: List of integers where each value represents the number of neurons in that layer.\n",
    "        - learning_rate: Learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize weights and biases for each layer using Xavier initialization\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layers) - 1):\n",
    "            # Xavier initialization\n",
    "            limit = np.sqrt(6 / (layers[i] + layers[i + 1]))\n",
    "            self.weights.append(np.random.uniform(-limit, limit, (layers[i], layers[i + 1])))\n",
    "            self.biases.append(np.zeros(layers[i + 1]))\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _sigmoid_deriv(self, x):\n",
    "        return self._sigmoid(x) * (1 - self._sigmoid(x))\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _relu_deriv(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        activations = [input_data]\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            z = np.dot(activations[i], self.weights[i]) + self.biases[i]\n",
    "            \n",
    "            if i == len(self.weights) - 1:\n",
    "                activation = self._sigmoid(z)  # Output layer\n",
    "            else:\n",
    "                activation = self._relu(z)  # Hidden layers\n",
    "                \n",
    "            activations.append(activation)\n",
    "        \n",
    "        return activations\n",
    "\n",
    "    def backpropagate(self, activations, target):\n",
    "        weight_gradients = [None] * len(self.weights)\n",
    "        bias_gradients = [None] * len(self.biases)\n",
    "        \n",
    "        # Output layer error and delta\n",
    "        error = activations[-1] - target\n",
    "        delta = error * self._sigmoid_deriv(activations[-1])\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            weight_gradients[i] = np.dot(activations[i].T, delta)\n",
    "            bias_gradients[i] = np.sum(delta, axis=0)\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self._relu_deriv(activations[i])\n",
    "        \n",
    "        return weight_gradients, bias_gradients\n",
    "\n",
    "    def update_parameters(self, weight_gradients, bias_gradients):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * weight_gradients[i]\n",
    "            self.biases[i] -= self.learning_rate * bias_gradients[i]\n",
    "\n",
    "    def train(self, input_data, target, epochs=10000):\n",
    "        for epoch in range(epochs):\n",
    "            activations = self.forward(input_data)\n",
    "            weight_gradients, bias_gradients = self.backpropagate(activations, target)\n",
    "            self.update_parameters(weight_gradients, bias_gradients)\n",
    "            \n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean((activations[-1] - target) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        activations = self.forward(input_data)\n",
    "        return activations[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85071b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T15:11:54.048341Z",
     "iopub.status.busy": "2024-10-16T15:11:54.047682Z",
     "iopub.status.idle": "2024-10-16T15:11:56.620216Z",
     "shell.execute_reply": "2024-10-16T15:11:56.619664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2546\n",
      "Epoch 1000, Loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000, Loss: 0.0003\n",
      "Epoch 3000, Loss: 0.0001\n",
      "Epoch 4000, Loss: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000, Loss: 0.0000\n",
      "Epoch 6000, Loss: 0.0000\n",
      "Epoch 7000, Loss: 0.0000\n",
      "Epoch 8000, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9000, Loss: 0.0000\n",
      "Epoch 10000, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11000, Loss: 0.0000\n",
      "Epoch 12000, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13000, Loss: 0.0000\n",
      "Epoch 14000, Loss: 0.0000\n",
      "Epoch 15000, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16000, Loss: 0.0000\n",
      "Epoch 17000, Loss: 0.0000\n",
      "Epoch 18000, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19000, Loss: 0.0000\n",
      "Epoch 20000, Loss: 0.0000\n",
      "Epoch 21000, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22000, Loss: 0.0000\n",
      "Epoch 23000, Loss: 0.0000\n",
      "Epoch 24000, Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[1.26783595e-04]\n",
      " [9.99963783e-01]\n",
      " [9.97712352e-01]\n",
      " [4.82532681e-05]]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "target = np.array([[0], [1], [1], [0]])  # XOR output\n",
    "\n",
    "# Create a neural network with 2 input neurons, 2 hidden layers with 4 neurons, and 1 output neuron\n",
    "nn = NeuralNetwork(layers=[2, 4, 4, 1], learning_rate=0.1)\n",
    "# Train the neural network\n",
    "nn.train(input_data, target, epochs=25000)\n",
    "    \n",
    "# Make predictions\n",
    "predictions = nn.predict(input_data)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2e9e5",
   "metadata": {},
   "source": [
    "<img title=\"a title\" alt=\"Alt text\" src=\"./nn_fc.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad7661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
